{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d122f041",
   "metadata": {},
   "source": [
    "# HW5 : NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88c6fe",
   "metadata": {},
   "source": [
    "**Goal.** In this homework you'll practice basic text processing: tokenization, normalization, simple stemming, and a dictionary-based sentiment analysis. You'll work with a sample review-like dataset and answer **5 questions**.\n",
    "\n",
    "**What you’ll do**\n",
    "1. Load a small dataset of short texts with gold sentiment labels.\n",
    "2. Tokenize and normalize the text (lowercasing, punctuation/number handling).\n",
    "3. Remove stopwords.\n",
    "4. Apply a rule-based stemmer (toy stemmer for learning).\n",
    "5. Build a dictionary-based sentiment scorer and evaluate it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e26a6",
   "metadata": {},
   "source": [
    "## 0) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81237399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define a dataset inline: a list of dicts with `text` and `label` in {`pos`, `neg`, `neu`}.\n",
    "# In a real assignment this might be loaded from a CSV.\n",
    "dataset = [\n",
    "    {\"text\": \"I loved the coffee and the sunny patio. Great vibes!\", \"label\": \"pos\"},\n",
    "    {\"text\": \"The service was slow and the food was cold. Not coming back.\", \"label\": \"neg\"},\n",
    "    {\"text\": \"Average experience. Some parts were fine, others meh.\", \"label\": \"neu\"},\n",
    "    {\"text\": \"Absolutely fantastic staff—friendly and helpful. Will recommend!\", \"label\": \"pos\"},\n",
    "    {\"text\": \"Terrible. Waited forever and the order was wrong.\", \"label\": \"neg\"},\n",
    "    {\"text\": \"It was okay overall; the dessert was nice but pricey.\", \"label\": \"neu\"},\n",
    "    {\"text\": \"So happy with the quick service and tasty sandwich.\", \"label\": \"pos\"},\n",
    "    {\"text\": \"Disappointed by the portion size and bland flavors.\", \"label\": \"neg\"},\n",
    "    {\"text\": \"Nothing special, but not bad either.\", \"label\": \"neu\"},\n",
    "    {\"text\": \"Amazing atmosphere, loved every minute there!\", \"label\": \"pos\"},\n",
    "    {\"text\": \"The coffee was amazing; I was loving it and quickly ordered seconds.\", \"label\": \"pos\"},\n",
    "    {\"text\": \"The sandwich tastes great but the soup was blandly seasoned.\", \"label\": \"neu\"},\n",
    "    {\"text\": \"We waited and waited; the order was wrong and slowly arriving.\", \"label\": \"neg\"},\n",
    "    {\"text\": \"Absolutely fantastic staff—friendly and helpful; I loved the service.\", \"label\": \"pos\"},\n",
    "    {\"text\": \"Not coming back—disappointing and overpriced.\", \"label\": \"neg\"},\n",
    "    {\"text\": \"Prices were okay, portions felt small; desserts were nicely presented.\", \"label\": \"neu\"},\n",
    "    {\"text\": \"The food was not good at all.\", \"label\": \"neg\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "len(dataset), dataset[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473120f7",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Write a simple tokenizer that:\n",
    "- lowercases text,\n",
    "- removes punctuation and digits,\n",
    "- splits on whitespace,\n",
    "- keeps only alphabetic tokens (a–z).\n",
    "\n",
    "**a)** After tokenizing all texts, what is the **vocabulary size** (number of unique tokens)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace anything not a-z with space\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    # Split on whitespace and filter empties\n",
    "    toks = [t for t in text.split() if t]\n",
    "    return toks\n",
    "\n",
    "# Apply tokenizer\n",
    "tokenized = [tokenize(item[\"text\"]) for item in dataset]\n",
    "tokenized[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d740a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q1 — Compute and print the vocabulary size.\n",
    "vocab = sorted({tok for doc in tokenized for tok in doc})\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "# Also, if helpful, print a few sample tokens:\n",
    "print(vocab[:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c79e07",
   "metadata": {},
   "source": [
    "### Normalization & Frequency\n",
    "Using your tokenized output, build a frequency table.\n",
    "\n",
    "**b)** What are the **top 5 most frequent tokens** across the corpus? (List them with counts.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ab07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "freq = Counter(tok for doc in tokenized for tok in doc)\n",
    "top5 = freq.most_common(n)\n",
    "print(\"Top-5 tokens:\", top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e4e20",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "**c)** We've provided a stopword list. Which two words in the list should not be in the stop words list and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\n",
    "    \"the\", \"and\", \"was\", \"is\", \"it\", \"but\", \"so\", \"with\", \"nice\", \"by\", \"a\", \"an\",\n",
    "    \"of\", \"to\", \"for\", \"pricey\"\n",
    "}\n",
    "\n",
    "def remove_stopwords(tokens: list[str]) -> list[str]:\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "tokenized_nostop = [remove_stopwords(doc) for doc in tokenized]\n",
    "Counter(tok for doc in tokenized_nostop for tok in doc).most_common(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962fef4",
   "metadata": {},
   "source": [
    "### Simple Stemmer\n",
    "Implement a very simple rule-based stemmer that strips a few common English suffixes: `ing`, `ed`, `ly`, `s` (applied in that order, once per token, only if the token is longer than the suffix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_stem(token: str) -> str:\n",
    "    # Apply in order: ing, ed, ly, s\n",
    "    for suf in [\"ing\", \"ed\", \"ly\", \"s\"]:\n",
    "        if token.endswith(suf) and len(token) > len(suf):\n",
    "            return token[: -len(suf)]\n",
    "    return token\n",
    "\n",
    "def stem_doc(tokens: list[str]) -> list[str]:\n",
    "    return [toy_stem(t) for t in tokens]\n",
    "\n",
    "# Choose whether to stem tokenized (with or without stopwords). We'll use the no-stop version here.\n",
    "stemmed = [stem_doc(doc) for doc in tokenized_nostop]\n",
    "vocab_before = sorted({tok for doc in tokenized_nostop for tok in doc})\n",
    "vocab_after = sorted({tok for doc in stemmed for tok in doc})\n",
    "print(\"Vocab size before stemming:\", len(vocab_before))\n",
    "print(\"Vocab size after stemming:\", len(vocab_after))\n",
    "print(\"Sample before→after:\", list(zip(vocab_before[:15], [toy_stem(t) for t in vocab_before[:15]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2114f",
   "metadata": {},
   "source": [
    "### Dictionary-Based Sentiment\n",
    "Below is a positive/negative lexicon for a very simple dictionary-based sentiment score:\n",
    "- Score(text) = (# of positive word hits) − (# of negative word hits).  \n",
    "- If the score > 0 → **pos**, score < 0 → **neg**, score == 0 → **neu**.\n",
    "\n",
    "> Note: This is intentionally simplistic and will misclassify some items. That's part of the learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e597cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lex = {\n",
    "   \"good\", \"love\",\"loved\",\"great\",\"fantastic\",\"friendly\",\"helpful\",\"recommend\",\"amazing\",\"happy\",\"tasty\",\"nice\",\"quick\",\"sunny\",\"vibe\",\"atmosphere\"\n",
    "}\n",
    "negative_lex = {\n",
    "    \"slow\",\"cold\", \"terrible\",\"waited\",\"wrong\",\"disappointed\",\"bland\",\"pricey\",\"forever\", \"bad\"\n",
    "}\n",
    "\n",
    "def sentiment_score(tokens: list[str]) -> int:\n",
    "    # count exact matches (pre-stemming tokens tend to work better for tiny lexicons)\n",
    "    pos_hits = sum(1 for t in tokens if t in positive_lex)\n",
    "    neg_hits = sum(1 for t in tokens if t in negative_lex)\n",
    "    return pos_hits - neg_hits\n",
    "\n",
    "def label_from_score(score: int) -> str:\n",
    "    if score > 0:\n",
    "        return \"pos\"\n",
    "    elif score < 0:\n",
    "        return \"neg\"\n",
    "    else:\n",
    "        return \"neu\"\n",
    "\n",
    "preds = []\n",
    "for toks, item in zip(tokenized, dataset):\n",
    "    s = sentiment_score(toks)\n",
    "    preds.append(label_from_score(s))\n",
    "\n",
    "list(zip([d[\"text\"] for d in dataset[:5]], [d[\"label\"] for d in dataset[:5]], preds[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef0659",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Compute **accuracy** of the dictionary-based classifier against the gold labels. Also compute a confusion matrix.\n",
    "\n",
    "**d:** What is the accuracy (0–1), and where do most errors occur (which true label vs predicted label)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "labels = [\"neg\",\"neu\",\"pos\"]  # fixed order for display\n",
    "\n",
    "gold = [d[\"label\"] for d in dataset]\n",
    "pred = preds\n",
    "\n",
    "# Accuracy\n",
    "acc = sum(g==p for g,p in zip(gold, pred)) / len(gold)\n",
    "print(\"Accuracy:\", round(acc, 3))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = { (g,p):0 for g in labels for p in labels }\n",
    "for g,p in zip(gold, pred):\n",
    "    cm[(g,p)] += 1\n",
    "\n",
    "# Pretty print\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "header = [\"\"] + labels\n",
    "rows = []\n",
    "for g in labels:\n",
    "    row = [g] + [cm[(g,p)] for p in labels]\n",
    "    rows.append(row)\n",
    "\n",
    "# Simple printout\n",
    "print(\"\\t\".join(header))\n",
    "for r in rows:\n",
    "    print(\"\\t\".join(str(x) for x in r))\n",
    "\n",
    "# TODO: Q4 — In a short sentence below, report the accuracy and where most errors happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print misclassified items: (text, gold label, predicted label)\n",
    "for d, g, p in zip(dataset, gold, pred):\n",
    "    if g != p:\n",
    "        print((d[\"text\"], g, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e8748",
   "metadata": {},
   "source": [
    "### Negation Handling\n",
    "Modify the scoring so that the presence of **`not`** reverses the next word’s polarity (only for the next single token). Then recompute predictions and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q4b — Implement a simple negation-aware scorer and compare accuracy\n",
    "def sentiment_score_negation(tokens: list[str]) -> int:\n",
    "    score = 0\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        t = tokens[i]\n",
    "        if t == \"not\" and i + 1 < len(tokens):\n",
    "            nxt = tokens[i+1]\n",
    "            if nxt in positive_lex:\n",
    "                score -= 1  # flip next positive to negative\n",
    "                i += 2\n",
    "                continue\n",
    "            elif nxt in negative_lex:\n",
    "                score += 1  # flip next negative to positive\n",
    "                i += 2\n",
    "                continue\n",
    "        # default behavior\n",
    "        if t in positive_lex:\n",
    "            score += 1\n",
    "        elif t in negative_lex:\n",
    "            score -= 1\n",
    "        i += 1\n",
    "    return score\n",
    "\n",
    "def predict_with(func):\n",
    "    return [label_from_score(func(toks)) for toks in tokenized]\n",
    "\n",
    "# Original accuracy from previous step is stored in `acc` (printed above).\n",
    "# Compute new accuracy with negation:\n",
    "pred_neg = predict_with(sentiment_score_negation)\n",
    "\n",
    "gold = [d[\"label\"] for d in dataset]\n",
    "acc_neg = sum(g==p for g,p in zip(gold, pred_neg)) / len(gold)\n",
    "\n",
    "print(\"Original accuracy (from earlier cell output): see above\")\n",
    "print(\"Negation-handling accuracy:\", round(acc_neg, 3))\n",
    "\n",
    "# Show side-by-side differences for quick inspection\n",
    "diffs = [(i, gold[i], preds[i], pred_neg[i], dataset[i][\"text\"]) for i in range(len(gold)) if preds[i] != pred_neg[i]]\n",
    "print(\"\\nCases where prediction changed due to negation handling:\")\n",
    "for i, g, p_old, p_new, txt in diffs:\n",
    "    print(f\"[{i}] GOLD={g} OLD={p_old} NEW={p_new} :: {txt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a9f62",
   "metadata": {},
   "source": [
    "**e:** Why is negation handling important in dictionary sentiment methods? Give an example from the dataset (or make a realistic example) where naive counting would fail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967af68",
   "metadata": {},
   "source": [
    "### Error Analysis \n",
    "Pick **3 misclassified** examples (if fewer than 3, pick all misclassified) and briefly explain **why** the dictionary approach failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7aeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = [(i, dataset[i][\"text\"], gold[i], pred[i]) for i in range(len(gold)) if gold[i] != pred[i]]\n",
    "print(\"Misclassified examples:\", len(mis))\n",
    "for i, text, g, p in mis:\n",
    "    print(f\"[{i}] GOLD={g} PRED={p} :: {text}\")\n",
    "# TODO: Q5 — Write your brief explanations below.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW8 Exploring Unfairness in Data ⚖️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unfairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is natural to assume that a model built from \"real-world\" data will inherently represent the world-at-large. We often take the data that we have for granted, especially when we are first getting started with Data Science. However, if we do not pay attention to what our data look like, how they were collected, and what features they contain, we may unknowingly create models that propagate cultural biases and unfairnesses.\n",
    "\n",
    "\n",
    "In 2014, Amazon began building programs that could automate the hiring process for engineers. They wanted a machine to be able to pick out the top resumes from the thousands they receive every year. They trained their model on all of the resumes that they had, hoping that the model would be able to identify trends in keyword frequency within those applications. If most applications contained the word \"intern,\" then one might reasonably expect that a resume containing it would be ranked higher than one that doesn't. However, as they began to deploy their model, it became increasingly apparent that the model was discriminating against women. When engineers investigated why this was the case, they found that the data they trained the model with, the resumes, had mostly come from men. The model had learned to prefer resumes that didn't contain the word \"women's\" because that word wasn't frequent seen during its training. Although gender was not explicitly a feature of the dataset, it was still present in the dataset, encoded within the experiences that applicants reported. \n",
    "\n",
    "Amazon's case serves as a reminder that we must be careful of our data, even more so today as data becomes cheaper to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Loan Approval Data\n",
    "\n",
    "Imagine that you are a data scientist at a bank and that one of your company's primary business areas is in lending money. The current loan approval process, that has been in place since the founding of the bank, has always relied on manual review of applications -- a process that is tedious and doesn't scale well in the modern age. The company wants to expand their business, but this archeic system is holding them back.\n",
    "\n",
    "Think about how to approach this problem, you immediately think of using the bank's past loan approval records to build a model that can learn how a human application reviewer decides which applications to approve and which to reject.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "\n",
    "assert exists(f'loan-payments.csv'), 'Loan data file is missing.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load our data. In the cell below, we read our [CSV][1] file into a [Pandas][2] [`DataFrame`][3] called `data`.\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Comma-separated_values\n",
    "[2]: https://pandas.pydata.org/\n",
    "[3]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(f'loan-payments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.a** How many examples are in our data set? How many features does it have?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.b** Come up with a description of what you think each feature is and what type of feature each one is. Which one should be our target variable? Which ones do you think will be useful for our model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Some Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop the columns in `data` that contain features that we are not interested in. Since `loan_id`s are not informative for predicting new loans, we can ignore them. Additionally, `effective_date`, `due_date`, and `paid_time_off` are all encoded in `past_due_days`. It is unlikely that the specifics of when a loan was due is predictive of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_interested = ['loan_id', 'effective_date', 'due_date', 'paid_off_time']\n",
    "\n",
    "data = data.drop(not_interested, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that `past_due_days` has `NaN` values?\n",
    "\n",
    "**Q2.a** Why might some of the values in `past_due_days` be `NaN`?  Discuss what we should do about these values and note your conclusion below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Replace the values in `past_due_days` with a reasonable value. `HINT` you can use the `fillna` function on `DataFrame`s to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'past_due_days' with 0, assuming that NaN means not past due\n",
    "data['past_due_days'] = data['past_due_days'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data Set\n",
    "\n",
    "Now that we have narrowed down the features we want to use, let's visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** For each feature, show the plot that we can use to understand the values of that feature. These plots should be appropriate for the type of each feature (e.g. use a bar plot for categorical features). Ensure that you have all the components off a nice plot, making sure to include things like axes labels, a legend, and a title. Also include a `raw` cell below each, describing what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this cell is free!\n",
    "\n",
    "# your code here\n",
    "# Plot for 'principal' (continuous feature)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(data['principal'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Principal')\n",
    "plt.xlabel('Principal')\n",
    "plt.ylabel('Number of Loans')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot for 'terms' (discrete numeric feature)\n",
    "plt.figure(figsize=(6,4))\n",
    "data['terms'].value_counts().sort_index().plot(kind='bar', color='salmon', edgecolor='black')\n",
    "plt.title('Distribution of Terms')\n",
    "plt.xlabel('Terms (days)')\n",
    "plt.ylabel('Number of Loans')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot for 'past_due_days' (may be highly skewed)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(data['past_due_days'], bins=20, color='seagreen', edgecolor='black')\n",
    "plt.title('Distribution of Past Due Days')\n",
    "plt.xlabel('Past Due Days')\n",
    "plt.ylabel('Number of Loans')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for 'age' (continuous feature)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(data['age'], bins=15, color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of Loans')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for 'education' (categorical)\n",
    "plt.figure(figsize=(6,4))\n",
    "data['education'].value_counts().plot(kind='bar', color='mediumpurple', edgecolor='black')\n",
    "plt.title('Distribution of Education Levels')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Number of Loans')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.b** Describe what you see in each of the plots above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "\n",
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Model\n",
    "\n",
    "Now that we have a sense for the nuances of our dataset we can try building some models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we will need to encode our categorical features with enumerations instead of the string values that they currently have. As a reminder, this is what our dataset looks like right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to do this encoding is to use the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) from `sklearn`. In the cell below, we create a list called `categorical` containing the names of the columns corresponding to the categorical features in our dataset. We then create and instance of a `LabelEncoder` and use it to transform the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical = ['loan_status', 'education', 'gender']\n",
    "\n",
    "# create an instance of a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# make a copy of our data\n",
    "encoded = data.copy()\n",
    "\n",
    "# apply the encoder's `fit_transform` method to the values for each categorical\n",
    "# feature column\n",
    "encoded[categorical] = data[categorical].apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the categorical values like \"PAIDOFF\" have now been replaced with numbers. We can see which numbers map to each value like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical:\n",
    "    print(*sorted(zip(encoded[column].unique(), data[column].unique()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's separate our features from our target variable, `loan_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encoded.loc[:, encoded.columns != 'loan_status'], encoded.loan_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a Baseline\n",
    "\n",
    "Now we're ready to start building models. First, let's create a train/test split of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's train and evaluate a LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** In the cell below, evaluate the model's performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.a** How does our model perform on the test set?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try looking at the model's performance on test examples of different genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "validation (men) score: {model.score(X_test[X_test['gender'] == 1], y_test[X_test['gender'] == 1]):0.3f}\n",
    "validation (women) score: {model.score(X_test[X_test['gender'] == 0], y_test[X_test['gender'] == 0]):0.3f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes!\n",
    "\n",
    "**Q3.b** What do you notice about these scores? How does these compare with the initial score we saw for the entire test set? What does this imply about our model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Gender\n",
    "\n",
    "So our model is biased with respect to gender and gender is a feature of the model. Would it help to ignore the gender feature during training? Let's try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating another train/test split, but this time using a copy of `X` and `y` that don't include `gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_without_gender = X.drop(['gender'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_without_gender, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `X_train` looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat our procedure for our baseline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'''\n",
    "\n",
    "validation score: {model.score(X_test, y_test)}\n",
    "validation (men) score: {model.score(X_test[X.iloc[X_test.index]['gender'] == 1],\n",
    "                                     y_test[X.iloc[X_test.index]['gender'] == 1]):0.3f}\n",
    "validation (women) score: {model.score(X_test[X.iloc[X_test.index]['gender'] == 0],\n",
    "                                       y_test[X.iloc[X_test.index]['gender'] == 0]):0.3f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.a** Did the results change?\n",
    "\n",
    "**Q4.b** Discuss what this might imply about our model and our data. \n",
    "\n",
    "**Q4.c** Also, discuss why it may or may not be a good idea to ignore \"protected variables\" like \"gender\" when training a model. Record your response below.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
